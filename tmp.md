# META / SYNDICATION

ðŸ”ï¸
* Warmerdam https://calmcode.io
* Willison

MANY SMALL REPOS
* facilitates sharing
* global dep cache prevents pileup
* prevent single repo from clutter
* repo tag for grep: `nmb` | `sandbox`

# â„ï¸ ABOUT

> gifs https://adamj.eu/contact/

* cv https://yorickpeterse.com/resume/
* phone screen http://127.0.0.1:1111/phone-screen/
* CV http://127.0.0.1:1111/cv
* quotes

# ðŸ—£ï¸ BLOG

ðŸ”ï¸ Julia Evans https://www.youtube.com/@wizardzines/videos
> the zines has been her only job since 2019

* addendum https://blog.plover.com/math/PM.html https://blog.plover.com/calendar/poor-richards-almanack.html
* factors: smart, wise, taste
* companies I want to work for https://www.wave.com/en/blog/world/

HOW THINGS WORK
* movies https://www.theringer.com/2024/8/21/24225522/the-arms-race-behind-where-movies-shoot https://www.theringer.com/2024/9/23/24252627/biggest-takeaways-netflix-data-dump-2024-streaming https://www.theringer.com/2024/9/25/24253629/how-to-make-your-own-tv-show-and-get-netflix-to-buy-it-mark-duplass-penelope-netflix https://www.theringer.com/2024/9/30/24258856/how-should-movie-and-tv-stars-be-paid
https://kaiwenwang.com/stack
https://gwern.net/matt-levine https://www.felixstocker.com/blog/geheimnisvoll https://news.ycombinator.com/item?id=41975993
https://gwern.net/book-writing

JOB BOARDS
* Github has a job profile?
* why did Stack Overflow kill their job board? why did they bring it back with Indeed?
* why aren't there tentpoles in this space akin to Hackernews and Lobste.rs?
> get set up with RSS btw and use for Lobste.rs

OBSESSIVES
* https://git-how.com/
* https://blog.plover.com/misc/three-corners-4.html
* bob is honest https://robertheaton.com/chatgpt/ https://uk.linkedin.com/in/robertjheaton
* https://en.wikipedia.org/wiki/Rosetta_Reitz

ZA
* EDI and Stedi: papering over a bad spec for fun and profit https://news.ycombinator.com/item?id=41919907
> kinda like Typescript to JS, OCaml to JS
* dev journal https://www.peterbaumgartner.com/blog/wrapping-a-rust-crate-in-a-python-package/
* how to fake being a pythonista in 2024
> I wish there was a good place to learn â€œthe other partsâ€ of C++, the build systems, using static analyzers, testing, dependency management, etc. https://news.ycombinator.com/item?id=34229802

## idea glossary

FASHION
* vibe shift
* matte vs. gloss
* high heels

## how i code

https://github.com/trimstray/the-book-of-secret-knowledge#black_small_square-network-dns
> https://uses.tech/ meta https://calmcode.io/
> changing stuff and seeing what happens https://orlybooks.com/
* profiling
* test
* docs
* API
* paradigms
* servers
* monitoring / metrics
* tracing
* analytics

## TIL

https://jvns.ca/blog/2024/11/09/new-microblog/
Lazygit multiple repos
https://news.ycombinator.com/item?id=42116800

LEARNING TO CODE FAILURES
* https://danluu.com/learning-to-program/
* DHH https://www.youtube.com/watch?v=9LfmrkyP81M

## opinions

### complaints

* Python pkg structure
* Markdown top-level headers
* EDI

### conferences

* PyCon: price, Youtube video URL change https://pycon-archive.python.org/2024/schedule/
* authority e.g. voting security talk
* Yegge visibility but also a way to fake

### my little world

https://gendignoux.com/readlist/#programming-languages

FAVORITES https://buttondown.com/hillelwayne/archive/in-defense-of/
* favorite software writing: Paul Ford what is code https://registerspill.thorstenball.com/p/joy-and-curiosity-8 https://www.youtube.com/watch?v=5WLlLxU2EZE
* blogroll https://claytonerrington.com/
* http://wmjas.wikidot.com/nabokov-s-recommendations

Klosterman/Simmons post-election
https://danluu.com/navigate-url/

* marginal contribution https://mattlakeman.org/2024/11/08/notes-on-guyana/

https://www.slowboring.com/p/a-tale-of-two-machines

* things I like: Vim, Python (pipx, bpython) visidata https://github.com/zachvalenta?tab=stars files https://alexdanco.com/2019/10/26/everything-is-amazing-but-nothing-is-ours/
* devs I like: Steve Yegge, Dan Luu, James Somers, Bob Heaton, Patrick McKenzie, Julia Evans https://www.scattered-thoughts.net/writing/what-is-the-point-of-an-online-conference/ https://registerspill.thorstenball.com/p/joy-and-curiosity-13

KEEPING UP
* how to - SO, YT, https://devdocs.io/
* general - HN, awesome-$foo, pods (SED, Python Podcast, Changelog) books (Manning, No Starch, Pragmatic)
* news - LWN weekly edition, Hacker Newsletter, newsletters (Python, Golang, Django)
* people - Thorsten, Luu, Somers, Patio11, Julia Evans, Jess Frazze, Yegge, PG

things outside my little world https://www.oneletterwords.com/weblog/?c=Restoring+the+Lost+Sense

### predictions

https://danluu.com/yegge-predictions/ https://danluu.com/butler-lampson-1999/
https://danluu.com/corrections/ https://danluu.com/bad-decisions/

python will get fast before anything else gets readable

### wrong on AI

OVERRATED
* https://news.ycombinator.com/item?id=30764701
* stagnation https://softwareengineeringdaily.com/2021/06/04/machine-learning-the-great-stagnation-with-mark-saroufim/
* ML hasn't solved radiology https://news.ycombinator.com/item?id=27422610
* as oracle https://marginalrevolution.com/marginalrevolution/2022/06/are-we-entering-an-age-of-oracles.html
* you mostly just need SQL + regression http://aiplaybook.a16z.com/docs/guides/dl https://news.ycombinator.com/item?id=25775872 https://www.youtube.com/watch?v=qw5dBdTXLEs https://ai.google/research/pubs/pub43146 https://nullprogram.com/blog/2020/11/24/
* job market has collapse for data science, big data, ML https://news.ycombinator.com/item?id=24330326 https://news.ycombinator.com/item?id=25775740
* ML doesn't think, only answers, and therefore can be hacked
> In 2017, M.I.T.â€™s LabSixâ€”a research group of undergraduate and graduate studentsâ€”succeeded in altering the pixels of a photograph of a cat so that, although it looked like a cat to human eyes, Inception became 99.99-per-cent sure it had been given a photograph of guacamole. (There was, it calculated, a slim chance that the photograph showed broccoli, or mortar.) Inception, of course, canâ€™t explain what features led it to conclude that a cat is a cat; as a result, thereâ€™s no easy way to predict how it might fail when presented with specially crafted or corrupted data. Such a system is likely to have unknown gaps in its accuracy that amount to vulnerabilities for a smart and determined attacker. - https://www.newyorker.com/tech/annals-of-technology/the-hidden-costs-of-automated-thinking
> Well, in the past, if a software engineer wanted to create a system to recognise something, they would write logical steps (â€˜rulesâ€™). To recognise a cat in a picture, you would write rules to find edges, fur, legs, eyes, pointed ears and so on, and bolt them all together and hope it worked. The trouble was that though this works in theory, in practice itâ€™s rather like trying to make a mechanical horse - itâ€™s theoretically possible, but the decree of complexity required is impractical. We canâ€™t actually describe all of the logical steps we use to walk, or to recognise a cat. With machine learning, instead of writing rules, you give examples (lots of examples) to a statistical engine, and that engine generates a model that can tell the difference. You give it 100,000 pictures labelled â€˜catâ€™ and 100,000 labelled â€˜no catâ€™ and the machine works out the difference. ML replaces hand-written logical steps with automatically determined patterns in data, and works much better for a very broad class of question - https://www.ben-evans.com/benedictevans/2018/12/19/does-ai-make-strong-tech-companies-stronger
* need data specific to problem
> First, though you need a lot of data for machine learning, the data you use is very specific to the problem that youâ€™re trying to solve. GE has lots of telemetry data from gas turbines, Google has lots of search data, and Amex has lots of credit card fraud data. You canâ€™t use the turbine data as examples to spot fraudulent transactions, and you canâ€™t use web searches to spot gas turbines that are about to fail. https://www.ben-evans.com/benedictevans/2018/12/19/does-ai-make-strong-tech-companies-stronger

# ðŸ“¹ HM

## draw
## music
## skate

# ðŸ“šï¸ MY BOOKS

## lyrics

* Babatunde, Luambo, Rochereau, Mulatu, Francis Bebey, Cesaria Evora, Amalia Rodrigues

## learn econ with Python

* learn econ with Python https://en.wikipedia.org/wiki/Paul_Romer

## history of thought

ðŸ“™
* history of private life
* Bryson short history of nearly everything

FIELDS
* math
* physics
* music
* linguistics
* religion
* violence
* music
* literature
* sociology
* psycholgy
* mythology
* folk tales

* https://www.amazon.com/Short-History-Nearly-Everything/dp/076790818X
* https://www.amazon.com/Ideas-History-Thought-Invention-Freud/dp/0060935642
* https://www.amazon.com/Ideas-History-Thought-Invention-Freud/dp/0060935642
* https://www.amazon.com/Brief-History-Thought-Philosophical-Learning/dp/0062074245
* https://www.amazon.com/History-Knowledge-Past-Present-Future/dp/0345373162

## ðŸ’¡ little ideas å¼„æ˜Žç™½

> what should the form factor be here? repo? notebook? animation?

* compilers https://www.achaq.dev/blog/artemis https://www.achaq.dev/blog/compilers
* financial exchange https://www.youtube.com/watch?v=b1e4t2k2KJY
* state machines https://www.achaq.dev/blog/distributed-systems-state-machines-special-relativity
* dependency injection
* parser generator with Bison
* ledger https://news.ycombinator.com/item?id=42269227
* dimensional analysis
* strength of record

### apple stat project

### crosstabs

```python
import pandas as pd

# Sample data: our exit poll data includes an overrepresentation of college-educated white voters
data = pd.DataFrame({
    'group': ['College-Educated White', 'All Other Voters'],
    'sample_size': [600, 400],  # Over-representation of college-educated white voters
    'biden_support': [0.65, 0.50]  # Biden support in each group
})

# Known election result for Biden (e.g., 52%)
actual_biden_support = 0.52

# Step 1: Calculate total Biden support from the unweighted sample
total_biden_votes_unweighted = sum(data['sample_size'] * data['biden_support'])
total_votes = sum(data['sample_size'])
biden_support_unweighted = total_biden_votes_unweighted / total_votes

print(f"Unweighted Biden support: {biden_support_unweighted:.2f}")

# Step 2: Calculate the necessary adjustment (weighting factor) to match actual election outcome
weighting_factor = actual_biden_support / biden_support_unweighted

# Step 3: Apply the weighting factor to the overrepresented group's support level
# We adjust the weight of each group to match the actual result
data['adjusted_biden_support'] = data['biden_support'] * weighting_factor

# Step 4: Calculate the new overall weighted Biden support
total_biden_votes_weighted = sum(data['sample_size'] * data['adjusted_biden_support'])
biden_support_weighted = total_biden_votes_weighted / total_votes

# Display the results
print(f"Weighted Biden support: {biden_support_weighted:.2f}")
print(data[['group', 'biden_support', 'adjusted_biden_support']])
```

### coverage

https://nedbatchelder.com/blog/202411/coveragepy_originally.html

### dimensional analysis

```python
import csv
from collections import defaultdict

# Example sales data (similar to previous example)
data = [
    {'Product': 'Laptop', 'Region': 'East', 'Sales': 10000},
    {'Product': 'Phone', 'Region': 'West', 'Sales': 8000},
    {'Product': 'Laptop', 'Region': 'East', 'Sales': 12000},
    {'Product': 'Tablet', 'Region': 'West', 'Sales': 5000},
    {'Product': 'Phone', 'Region': 'East', 'Sales': 9000}
]

# Dictionary to store aggregated sales
sales_summary = defaultdict(int)

# Aggregate sales by Product and Region
for record in data:
    product = record['Product']
    region = record['Region']
    sales = record['Sales']
    # Using a tuple (product, region) as key to aggregate sales
    sales_summary[(product, region)] += sales

# Print the aggregated sales
for (product, region), total_sales in sales_summary.items():
    print(f"Product: {product}, Region: {region}, Total Sales: {total_sales}")
```

### Levenshtein distance

```python
def levenshtein_distance(s1: str, s2: str) -> int:
    """
    Compute the Levenshtein distance between two strings.

    :param s1: First string
    :param s2: Second string
    :return: Levenshtein distance
    """
    # Initialize matrix with size (len(s1)+1) x (len(s2)+1)
    rows, cols = len(s1) + 1, len(s2) + 1
    dp = [[0] * cols for _ in range(rows)]

    # Fill the first row and column with indices
    for i in range(rows):
        dp[i][0] = i
    for j in range(cols):
        dp[0][j] = j

    # Fill the rest of the matrix
    for i in range(1, rows):
        for j in range(1, cols):
            cost = 0 if s1[i - 1] == s2[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,    # Deletion
                dp[i][j - 1] + 1,    # Insertion
                dp[i - 1][j - 1] + cost  # Substitution
            )

    return dp[-1][-1]


# Example usage
if __name__ == "__main__":
    str1 = "kitten"
    str2 = "sitting"
    distance = levenshtein_distance(str1, str2)
    print(f"Levenshtein distance between '{str1}' and '{str2}': {distance}")
```

### markov chains

### PageRank

```python
from collections import defaultdict

def pagerank(graph, damping_factor=0.85, iterations=100):
    # Initialize PageRank values to 1/N
    nodes = list(graph.keys())
    N = len(nodes)
    ranks = {node: 1 / N for node in nodes}
    
    # Run the PageRank algorithm
    for _ in range(iterations):
        new_ranks = defaultdict(float)
        for node in nodes:
            # Distribute current rank to outbound links
            outbound_links = graph[node]
            if outbound_links:
                shared_rank = ranks[node] / len(outbound_links)
                for outbound in outbound_links:
                    new_ranks[outbound] += shared_rank
            # Handle dangling nodes (no outbound links)
            else:
                for other_node in nodes:
                    new_ranks[other_node] += ranks[node] / N

        # Apply the damping factor
        for node in nodes:
            new_ranks[node] = (1 - damping_factor) / N + damping_factor * new_ranks[node]
        
        ranks = new_ranks

    return ranks

# Example directed graph
# Each key is a node, and its value is a list of nodes it links to
example_graph = {
    "A": ["B", "C"],
    "B": ["C", "D"],
    "C": ["A"],
    "D": ["C"],
    "E": ["A", "D"],
}

# Run the toy PageRank implementation
result = pagerank(example_graph)

# Print the results
print("PageRank results:")
for page, rank in sorted(result.items(), key=lambda x: x[1], reverse=True):
    print(f"{page}: {rank:.4f}")
```

### telemetry vocab

# ðŸ““ MY NOTES ON OTHERS BOOKS

https://daverupert.com/bookshelf/

# ðŸ”‘ PERSONAL

## photos
## updates
