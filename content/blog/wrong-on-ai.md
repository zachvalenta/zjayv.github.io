+++
title = "wrong on Ai"
date = 2024-12-12
+++

OVERRATED
* https://news.ycombinator.com/item?id=30764701
* stagnation https://softwareengineeringdaily.com/2021/06/04/machine-learning-the-great-stagnation-with-mark-saroufim/
* ML hasn't solved radiology https://news.ycombinator.com/item?id=27422610
* as oracle https://marginalrevolution.com/marginalrevolution/2022/06/are-we-entering-an-age-of-oracles.html
* you mostly just need SQL + regression http://aiplaybook.a16z.com/docs/guides/dl https://news.ycombinator.com/item?id=25775872 https://www.youtube.com/watch?v=qw5dBdTXLEs https://ai.google/research/pubs/pub43146 https://nullprogram.com/blog/2020/11/24/
* job market has collapse for data science, big data, ML https://news.ycombinator.com/item?id=24330326 https://news.ycombinator.com/item?id=25775740
* ML doesn't think, only answers, and therefore can be hacked
> In 2017, M.I.T.’s LabSix—a research group of undergraduate and graduate students—succeeded in altering the pixels of a photograph of a cat so that, although it looked like a cat to human eyes, Inception became 99.99-per-cent sure it had been given a photograph of guacamole. (There was, it calculated, a slim chance that the photograph showed broccoli, or mortar.) Inception, of course, can’t explain what features led it to conclude that a cat is a cat; as a result, there’s no easy way to predict how it might fail when presented with specially crafted or corrupted data. Such a system is likely to have unknown gaps in its accuracy that amount to vulnerabilities for a smart and determined attacker. - https://www.newyorker.com/tech/annals-of-technology/the-hidden-costs-of-automated-thinking
> Well, in the past, if a software engineer wanted to create a system to recognise something, they would write logical steps (‘rules’). To recognise a cat in a picture, you would write rules to find edges, fur, legs, eyes, pointed ears and so on, and bolt them all together and hope it worked. The trouble was that though this works in theory, in practice it’s rather like trying to make a mechanical horse - it’s theoretically possible, but the decree of complexity required is impractical. We can’t actually describe all of the logical steps we use to walk, or to recognise a cat. With machine learning, instead of writing rules, you give examples (lots of examples) to a statistical engine, and that engine generates a model that can tell the difference. You give it 100,000 pictures labelled ‘cat’ and 100,000 labelled ‘no cat’ and the machine works out the difference. ML replaces hand-written logical steps with automatically determined patterns in data, and works much better for a very broad class of question - https://www.ben-evans.com/benedictevans/2018/12/19/does-ai-make-strong-tech-companies-stronger
* need data specific to problem
> First, though you need a lot of data for machine learning, the data you use is very specific to the problem that you’re trying to solve. GE has lots of telemetry data from gas turbines, Google has lots of search data, and Amex has lots of credit card fraud data. You can’t use the turbine data as examples to spot fraudulent transactions, and you can’t use web searches to spot gas turbines that are about to fail. https://www.ben-evans.com/benedictevans/2018/12/19/does-ai-make-strong-tech-companies-stronger

